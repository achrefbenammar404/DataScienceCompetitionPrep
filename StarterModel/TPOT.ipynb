{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical TPOT runs will take hours to days to finish (unless it's a small dataset), but you can always interrupt the run partway through and see the best results so far. TPOT also provides a warm_start parameter that lets you restart a TPOT run from where it left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit this web page to know how to use !!!\n",
    "http://epistasislab.github.io/tpot/using/ -> classification and instructions \n",
    "http://epistasislab.github.io/tpot/examples/ -> different examples with various problems from classification to regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can run TPOT for only a few minutes and it will find a reasonably good pipeline for your dataset. However, if you don't run TPOT for long enough, it may not find the best possible pipeline for your dataset. It may even not find any suitable pipeline at all, in which case a RuntimeError('A pipeline has not yet been optimized. Please call fit() first.') will be raised. Often it is worthwhile to run multiple instances of TPOT in parallel for a long time (hours to days) to allow TPOT to thoroughly search the pipeline space for your dataset.\n",
    "\n",
    "AUTOML ALGORITHMS CAN TAKE A LONG TIME TO FINISH THEIR SEARCH\n",
    "AutoML algorithms aren't as simple as fitting one model on the dataset; they are considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with multiple preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyperparameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline.\n",
    "\n",
    "As such, TPOT will take a while to run on larger datasets, but it's important to realize why. With the default TPOT settings (100 generations with 100 population size), TPOT will evaluate 10,000 pipeline configurations before finishing. To put this number into context, think about a grid search of 10,000 hyperparameter combinations for a machine learning algorithm and how long that grid search will take. That is 10,000 model configurations to evaluate with 10-fold cross-validation, which means that roughly 100,000 models are fit and evaluated on the training data in one grid search. That's a time-consuming procedure, even for simpler models like decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPOT with code\n",
    "We've taken care to design the TPOT interface to be as similar as possible to scikit-learn.\n",
    "\n",
    "TPOT can be imported just like any regular Python module. To import TPOT, type:\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "then create an instance of TPOT as follows:\n",
    "\n",
    "pipeline_optimizer = TPOTClassifier()\n",
    "It's also possible to use TPOT for regression problems with the TPOTRegressor class. Other than the class name, a TPOTRegressor is used the same way as a TPOTClassifier. You can read more about the TPOTClassifier and TPOTRegressor classes in the API documentation.\n",
    "\n",
    "Some example code with custom TPOT parameters might look like:\n",
    "\n",
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "Now TPOT is ready to optimize a pipeline for you. You can tell TPOT to optimize a pipeline based on a data set with the fit function:\n",
    "\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "The fit function initializes the genetic programming algorithm to find the highest-scoring pipeline based on average k-fold cross-validation Then, the pipeline is trained on the entire set of provided samples, and the TPOT instance can be used as a fitted model.\n",
    "\n",
    "You can then proceed to evaluate the final pipeline on the testing set with the score function:\n",
    "\n",
    "print(pipeline_optimizer.score(X_test, y_test))\n",
    "Finally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the export function:\n",
    "\n",
    "pipeline_optimizer.export('tpot_exported_pipeline.py')\n",
    "Once this code finishes running, tpot_exported_pipeline.py will contain the Python code for the optimized pipeline.\n",
    "\n",
    "Below is a full example script using TPOT to optimize a pipeline, score it, and export the best pipeline to a file.\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))\n",
    "pipeline_optimizer.export('tpot_exported_pipeline.py')\n",
    "Check our examples to see TPOT applied to some specific data sets.\n",
    "\n",
    "TPOT on the command line\n",
    "To use TPOT via the command line, enter the following command with a path to the data file:\n",
    "\n",
    "tpot /path_to/data_file.csv\n",
    "An example command-line call to TPOT may look like:\n",
    "\n",
    "tpot data/mnist.csv -is , -target class -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
